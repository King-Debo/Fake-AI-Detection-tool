# Import Flask and its extensions
from backend import Flask, request, render_template, jsonify
from flask_uploads import UploadSet, configure_uploads, ALL
from flask_cors import CORS
from flask_socketio import SocketIO

# Import PyTorch and its modules
import torch
import torchvision
import torchaudio
import torchtext

# Import Hugging Face and its models
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Import other libraries and packages
import os
import requests
import json
import random

# Create a Flask app instance
app = Flask(__name__)

# Enable cross-origin resource sharing
CORS(app)

# Create a Socket.IO instance for real-time communication
socketio = SocketIO(app)

# Configure the app secret key and the upload folder
app.config["SECRET_KEY"] = os.urandom(24)
app.config["UPLOADED_MEDIA_DEST"] = "static/uploads"

# Create an UploadSet instance for handling file uploads
media = UploadSet("media", ALL)

# Configure the app with the UploadSet instance
configure_uploads(app, media)

# Define a function that loads a pre-trained model from Hugging Face's model hub or other sources
def load_model(model_name):
    # Check if the model name is valid and supported
    if model_name in ["FaceForensics++", "Grover", "DeFakeHop", "FakeCatcher"]:
        # Load the model and the tokenizer from Hugging Face's model hub using AutoModelForSequenceClassification and AutoTokenizer classes
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        # Return the model and the tokenizer as a tuple
        return (model, tokenizer)
    else:
        # Raise an exception if the model name is invalid or unsupported
        raise Exception(f"Invalid or unsupported model name: {model_name}")

# Define a function that performs the fake AI content detection using deep learning techniques
def detect_fake_ai_content(media_type, media_file):
    # Check if the media type is valid and supported
    if media_type in ["video", "image", "text", "voice", "music"]:
        # Choose a pre-trained model based on the media type
        if media_type == "video" or media_type == "image":
            # Use FaceForensics++ for detecting deepfake videos or images of faces
            model_name = "FaceForensics++"
        elif media_type == "text":
            # Use Grover for detecting fake text generated by GPT-2 or GPT-3 models
            model_name = "Grover"
        elif media_type == "voice":
            # Use DeFakeHop for detecting fake voice generated by WaveNet or WaveRNN models
            model_name = "DeFakeHop"
        elif media_type == "music":
            # Use FakeCatcher for detecting fake music generated by Jukebox or MuseNet models
            model_name = "FakeCatcher"
        # Load the pre-trained model and the tokenizer from Hugging Face's model hub or other sources using the load_model function
        model, tokenizer = load_model(model_name)
        # Process the media file according to the media type and the model requirements
        if media_type == "video" or media_type == "image":
            # Use torchvision's transforms to convert the media file into a tensor and resize it to 224x224 pixels
            transform = torchvision.transforms.Compose([
                torchvision.transforms.ToTensor(),
                torchvision.transforms.Resize((224, 224))
            ])
            tensor = transform(media_file)
            # Add a batch dimension to the tensor and move it to the device (CPU or GPU) where the model is located
            tensor = tensor.unsqueeze(0).to(model.device)
            # Pass the tensor to the model and get the output logits
            logits = model(tensor)[0]
        elif media_type == "text":
            # Use tokenizer's encode_plus method to convert the media file (which is a text) into input ids and attention mask tensors
            encoding = tokenizer.encode_plus(
                media_file,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding="max_length"
            )
            input_ids = encoding["input_ids"]
            attention_mask = encoding["attention_mask"]
            # Move the tensors to the device (CPU or GPU) where the model is located
            input_ids = input_ids.to(model.device)
            attention_mask = attention_mask.to(model.device)
            # Pass the tensors to the model and get the output logits
            logits = model(input_ids, attention_mask)[0]
        elif media_type == "voice" or media_type == "music":
            # Use torchaudio's load method to convert the media file into a waveform and a sample rate tensors
            waveform, sample_rate = torchaudio.load(media_file)
            # Resample the waveform to 16 kHz if the sample rate is different
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
                sample_rate = 16000
            # Add a batch dimension to the waveform and move it to the device (CPU or GPU) where the model is located
            waveform = waveform.unsqueeze(0).to(model.device)
            # Pass the waveform and the sample rate to the model and get the output logits
            logits = model(waveform, sample_rate)[0]
        # Apply softmax function to the logits to get the probability distribution
        probabilities = torch.nn.functional.softmax(logits, dim=-1)
        # Get the probability of the media being fake or real by taking the maximum value of the probabilities
        probability, label = probabilities.max(dim=-1)
        # Convert the probability and the label tensors into Python scalars
        probability = probability.item()
        label = label.item()
        # Check if the label is 0 or 1, which corresponds to fake or real respectively
        if label == 0:
            # Set the type and the explanation variables based on the media type and the model name
            type = f"Fake {media_type.capitalize()}"
            explanation = f"This {media_type} is detected as fake by {model_name}, a pre-trained model that uses deep learning techniques to classify fake AI content."
        elif label == 1:
            # Set the type and the explanation variables to None, indicating that the media is real
            type = None
            explanation = None
        # Return the probability, the type, and the explanation as a tuple
        return (probability, type, explanation)
    else:
        # Raise an exception if the media type is invalid or unsupported
        raise Exception(f"Invalid or unsupported media type: {media_type}")

# Define a route for rendering the index page using Flask's render_template function
@app.route("/")
def index():
    return render_template("index.html")

# Define a route for handling the fake AI content detection using Flask's request and jsonify functions
@app.route("/detect", methods=["POST"])
def detect():
    try:
        # Get the media type and the media file from the request form data
        media_type = request.form.get("media-type")
        media_file = request.files.get("media-file")
        # Save the media file in the upload folder using Flask-Uploads' save method
        filename = media.save(media_file)
        # Get the full path of the saved media file
        filepath = os.path.join(app.config["UPLOADED_MEDIA_DEST"], filename)
        # Perform the fake AI content detection using the detect_fake_ai_content function
        probability, type, explanation = detect_fake_ai_content(media_type, filepath)
        # Return a JSON response that contains the probability, the type, and the explanation as keys and values
        return jsonify({
            "probability": probability,
            "type": type,
            "explanation": explanation
        })
    except Exception as e:
        # Return a JSON response that contains an error message if an exception occurs
        return jsonify({
            "error": str(e)
        })

# Run the app with Socket.IO's run method
if __name__ == "__main__":
    socketio.run(app)
